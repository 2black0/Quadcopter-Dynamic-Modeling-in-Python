{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the quadcopter environment for reinforcement learning training.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from quadcopter.gym_env import QuadcopterGymEnv\n",
    "\n",
    "# Check if gymnasium is available\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    print(\"Gymnasium is available\")\n",
    "except ImportError:\n",
    "    print(\"Gymnasium not available. Please install with: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "Let's create a quadcopter environment for RL training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = QuadcopterGymEnv(dt=0.02, max_steps=1000)\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Policy Example\n",
    "\n",
    "Let's create a simple policy for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy:\n",
    "    \"\"\"A simple policy that outputs fixed motor speeds.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Base hover speed\n",
    "        self.hover_speed = np.sqrt(0.65 * 9.81 / (4 * 3.25e-5))\n",
    "    \n",
    "    def predict(self, observation):\n",
    "        \"\"\"Predict action based on observation.\"\"\"\n",
    "        # Simple policy: if too low, increase thrust; if too high, decrease thrust\n",
    "        z_pos = observation[2]  # Z position\n",
    "        target_z = 1.0  # Target height\n",
    "        \n",
    "        # Adjust motor speeds based on position error\n",
    "        error = target_z - z_pos\n",
    "        adjustment = error * 10.0  # Simple proportional control\n",
    "        \n",
    "        # Apply adjustment to all motors\n",
    "        action = np.full(4, self.hover_speed + adjustment)\n",
    "        \n",
    "        # Ensure motor speeds are within bounds\n",
    "        action = np.clip(action, 0.0, 1000.0)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's run a simple training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple policy\n",
    "policy = SimplePolicy()\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 3\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    step_count = 0\n",
    "    \n",
    "    print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from policy\n",
    "        action = policy.predict(obs)\n",
    "        \n",
    "        # Take step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Accumulate reward\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        # Print progress every 100 steps\n",
    "        if step_count % 100 == 0:\n",
    "            print(f\"  Step {step_count}: Z={obs[2]:.2f}m, Reward={reward:.2f}\")\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"  Episode finished after {step_count} steps\")\n",
    "    print(f\"  Total reward: {total_reward:.2f}\")\n",
    "    print(f\"  Final Z position: {obs[2]:.2f}m\")\n",
    "    print()\n",
    "\n",
    "# Print summary\n",
    "mean_reward = np.mean(episode_rewards)\n",
    "std_reward = np.std(episode_rewards)\n",
    "print(\"Training Summary:\")\n",
    "print(f\"  Mean reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "print(f\"  Best episode: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"  Worst episode: {np.min(episode_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_rewards, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('RL Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. How to create a quadcopter RL environment\n",
    "2. How to implement a simple policy\n",
    "3. How to run a training loop\n",
    "4. How to visualize training progress\n",
    "\n",
    "For more advanced RL training, you can integrate with libraries like Stable Baselines3 or Ray RLlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}